{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poe_BOT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOwsuGQQY9OL",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pylt5qZYsWPh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d9a8a59c-ec60-4254-c65c-e6bc21791539"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-13 18:09:36--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.217.128, 2607:f8b0:400c:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.217.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-06-13 18:09:36 (101 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9vH8Y59ajYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "345fcc4f-050c-4687-8c3b-b76df4349640"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(predictors, label, epochs=50, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "484/484 [==============================] - 53s 110ms/step - loss: 6.9057 - accuracy: 0.0204\n",
            "Epoch 2/50\n",
            "484/484 [==============================] - 53s 110ms/step - loss: 6.5033 - accuracy: 0.0217\n",
            "Epoch 3/50\n",
            "484/484 [==============================] - 53s 110ms/step - loss: 6.3972 - accuracy: 0.0246\n",
            "Epoch 4/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 6.2743 - accuracy: 0.0298\n",
            "Epoch 5/50\n",
            "484/484 [==============================] - 52s 107ms/step - loss: 6.1787 - accuracy: 0.0355\n",
            "Epoch 6/50\n",
            "484/484 [==============================] - 53s 109ms/step - loss: 6.1000 - accuracy: 0.0395\n",
            "Epoch 7/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 6.0214 - accuracy: 0.0391\n",
            "Epoch 8/50\n",
            "484/484 [==============================] - 57s 118ms/step - loss: 5.9401 - accuracy: 0.0427\n",
            "Epoch 9/50\n",
            "484/484 [==============================] - 53s 109ms/step - loss: 5.8403 - accuracy: 0.0501\n",
            "Epoch 10/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 5.7195 - accuracy: 0.0566\n",
            "Epoch 11/50\n",
            "484/484 [==============================] - 52s 107ms/step - loss: 5.6023 - accuracy: 0.0621\n",
            "Epoch 12/50\n",
            "484/484 [==============================] - 53s 109ms/step - loss: 5.4845 - accuracy: 0.0709\n",
            "Epoch 13/50\n",
            "484/484 [==============================] - 52s 107ms/step - loss: 5.3712 - accuracy: 0.0797\n",
            "Epoch 14/50\n",
            "484/484 [==============================] - 52s 107ms/step - loss: 5.2594 - accuracy: 0.0862\n",
            "Epoch 15/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 5.1477 - accuracy: 0.0946\n",
            "Epoch 16/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 5.0412 - accuracy: 0.1019\n",
            "Epoch 17/50\n",
            "484/484 [==============================] - 53s 109ms/step - loss: 4.9333 - accuracy: 0.1081\n",
            "Epoch 18/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 4.8340 - accuracy: 0.1171\n",
            "Epoch 19/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 4.7276 - accuracy: 0.1241\n",
            "Epoch 20/50\n",
            "484/484 [==============================] - 57s 117ms/step - loss: 4.6323 - accuracy: 0.1363\n",
            "Epoch 21/50\n",
            "484/484 [==============================] - 52s 107ms/step - loss: 4.5317 - accuracy: 0.1427\n",
            "Epoch 22/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 4.4270 - accuracy: 0.1572\n",
            "Epoch 23/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 4.3270 - accuracy: 0.1697\n",
            "Epoch 24/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 4.2319 - accuracy: 0.1791\n",
            "Epoch 25/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 4.1284 - accuracy: 0.1921\n",
            "Epoch 26/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 4.0308 - accuracy: 0.2066\n",
            "Epoch 27/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.9409 - accuracy: 0.2209\n",
            "Epoch 28/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.8455 - accuracy: 0.2370\n",
            "Epoch 29/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 3.7540 - accuracy: 0.2533\n",
            "Epoch 30/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.6561 - accuracy: 0.2731\n",
            "Epoch 31/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.5772 - accuracy: 0.2856\n",
            "Epoch 32/50\n",
            "484/484 [==============================] - 56s 116ms/step - loss: 3.4896 - accuracy: 0.3054\n",
            "Epoch 33/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.4086 - accuracy: 0.3251\n",
            "Epoch 34/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.3365 - accuracy: 0.3405\n",
            "Epoch 35/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.2513 - accuracy: 0.3567\n",
            "Epoch 36/50\n",
            "484/484 [==============================] - 52s 107ms/step - loss: 3.1653 - accuracy: 0.3796\n",
            "Epoch 37/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.0941 - accuracy: 0.3945\n",
            "Epoch 38/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 3.0302 - accuracy: 0.4120\n",
            "Epoch 39/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 2.9531 - accuracy: 0.4287\n",
            "Epoch 40/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 2.8809 - accuracy: 0.4450\n",
            "Epoch 41/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 2.8235 - accuracy: 0.4565\n",
            "Epoch 42/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 2.7529 - accuracy: 0.4709\n",
            "Epoch 43/50\n",
            "484/484 [==============================] - 56s 116ms/step - loss: 2.6828 - accuracy: 0.4864\n",
            "Epoch 44/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 2.6356 - accuracy: 0.4999\n",
            "Epoch 45/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 2.5838 - accuracy: 0.5090\n",
            "Epoch 46/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 2.5267 - accuracy: 0.5249\n",
            "Epoch 47/50\n",
            "484/484 [==============================] - 51s 106ms/step - loss: 2.4675 - accuracy: 0.5367\n",
            "Epoch 48/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 2.4262 - accuracy: 0.5490\n",
            "Epoch 49/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 2.3575 - accuracy: 0.5623\n",
            "Epoch 50/50\n",
            "484/484 [==============================] - 51s 105ms/step - loss: 2.3084 - accuracy: 0.5772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvBJSZYbLwpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "6f2af695-6f81-46e4-957e-c7ebb653e356"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d24ff6072e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCdONUzpL0Kl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6654fd2f-c631-4c06-d190-5f6ddbffa07e"
      },
      "source": [
        "seed_text = \"Shall I compare you to a summer's day\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-18bdbb3065b1>:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "Shall I compare you to a summer's day clearer light light light dyed burn'd gems dyed grow bettering days of time hate part to despise spend reap hate part to sell spend sort forth thee bright tender'd green thee up me live bright hence rare strong shade part on shown cross twain grow days live bright room bad new old report lies slain ' live old strong worth alone who so told me so so dearly know it so virtuous know good untrue mine eyes be thine more alone so blessed free thy years sorrow wrong light dyed dyed dyed burn'd dyed torn grow days hid me up\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}